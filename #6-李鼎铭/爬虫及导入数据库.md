# 爬虫

## 配置python环境

在官网上找到：

![在这里插入图片描述](https://img-blog.csdnimg.cn/47dd83eb9f1d423cb03f4ee8a81624b2.png#pic_center)

在安装过程中就是别忘了添加环境变量就好，之后在电脑上查询python是否安装好。

![在这里插入图片描述](https://img-blog.csdnimg.cn/0c8bfd04184b4df594745ff6c32dd90d.png#pic_center)

## 下载Pycharm

之后下载pycharm

![在这里插入图片描述](https://img-blog.csdnimg.cn/128c36a148704f23a12745354aa48f87.png#pic_center)

当然是社区免费版了哈

之后在配置好后准备本次爬虫需要的库，本次我就使用requests了

![在这里插入图片描述](https://img-blog.csdnimg.cn/80e008b410fd494ba5ecb34c2499f3ab.png#pic_center)

## 开始爬取

到需要爬取的网站中寻找url，本次需要爬取评论者的名字，评论时间和内容

![在这里插入图片描述](https://img-blog.csdnimg.cn/8b154d8bfcb949d2b86675ac044c612b.png#pic_center)

先点击检查，刷新页面，再再网络选项中调出来搜索框，复制一段评论，之后就可以找到评论所属于的url了

![在这里插入图片描述](https://img-blog.csdnimg.cn/2cc4d6a1ec3040bba1029aad26c91174.png#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/b3c9debfb4c944a38168b3dd17fc9f48.png#pic_center)

顺带拿一个user-agent

![在这里插入图片描述](https://img-blog.csdnimg.cn/6dad78d201ef400caa81ab0d3adc8bf0.png#pic_center)

复制url进行检查

![在这里插入图片描述](https://img-blog.csdnimg.cn/e4d144d5ca6c413385e92d8555bd810d.png#pic_center)

发现没有问题

之后根据网上提示发现去掉url中的callback=fetchJSON_comment98&&照样可以打开网站而且把网页中的内容变成字典形式，所以之后提取东西就简单了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c7bec609cb8e48119a70f11493bd4194.png#pic_center)

之后在pycharm中编写代码，输入去掉json形式的url之后进行提取，由于需要提取500条评论，而京东一页只有10条评论，所以将pagenum设为变量，url中的page=即代表是多少页，所以在循环中设定到（0，51）由前闭后开原则就可以查询到510条评论。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cc1c1757f71c40a085537aab7a14e28d.png#pic_center)

## 保存到本地

使用fp=open（）函数，并用‘a’模式来追加，之后就可以保存到本地了。

```python
   fp = open("E:\Pythoncharm\爬虫jiaotang\success.txt", "a", encoding='utf-8')
            fp.write(str(i)+"   ")
            fp.write(nickname+"\t")
            fp.write(creationTime+"\n")
            fp.write(content+"\n")
            time.sleep(0.4)
            fp.close()
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/2f67b44ddc804eca9511352cc47dafad.png#pic_center)

之后就是将文档形式转为excel表格。

![在这里插入图片描述](https://img-blog.csdnimg.cn/140900cfabad4ecf86b623c373b12c37.png#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/c3d508fc6c43474a925355a13926e297.png#pic_center)

语言也就是：

```
fp = open("E:\Pythoncharm\爬虫jiaotang\success.txt", "a", encoding='utf-8')
            fp.write(str(i)+"   ")
            fp.write(nickname+"\t")
            fp.write(creationTime+"\n")
            fp.write(content+"\n")
            time.sleep(0.4)
            fp.close()
```

# 导入数据库



首先是安装MySQL，官网中寻找：

![在这里插入图片描述](https://img-blog.csdnimg.cn/eafb221ab973427aa908675d320d9bd5.png#pic_center)

之后通过MySQL带来的命令窗口来进行操作，先输入密码，之后操作。

![在这里插入图片描述](https://img-blog.csdnimg.cn/78e602a83a6741e1857533c067757c67.png#pic_center)

之后创建数据库jiaotang

`create database jiaotang`

之后就是显示数据库：

![在这里插入图片描述](https://img-blog.csdnimg.cn/13d5a4f6ad6a462b8df50f6f226a53a4.png#pic_center)

看起来没问题，之后下载了一个图形化界面工具DataGrip。

在datagrip中选择Mysql，下载驱动架包，应用之后就可以看到我的数据库了

![在这里插入图片描述](https://img-blog.csdnimg.cn/8b127f6621d44ec297bc84f93a925528.png#pic_center)

之后将excel表格后缀改为csv

![在这里插入图片描述](https://img-blog.csdnimg.cn/637ada32c61642d4a1725202c119a701.png#pic_center)

之后就是在Datagrip中导入jiaotang的数据库中

![在这里插入图片描述](https://img-blog.csdnimg.cn/f26f42bf448f4bb797828bbe278ce83c.png#pic_center)

之后就成功喽

![在这里插入图片描述](https://img-blog.csdnimg.cn/77538f5c1c1846c88f787aea87ce5a8b.png#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/75f6bbd88c694d8cb61752d4691249ed.png#pic_center)
